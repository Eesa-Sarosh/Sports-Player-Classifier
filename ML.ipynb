{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c6be69-0732-4843-8c8e-8cc156049d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import matplotlib\n",
    "import shutil\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1abbe-2899-44c6-a7d0-87d7138be6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "import requests\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "\n",
    "def download_image(url, folder_name, num):\n",
    "\n",
    "    # write image to file\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code == 200:\n",
    "        with open(os.path.join(folder_name, str(num)+\".jpg\"), 'wb') as file:\n",
    "            file.write(reponse.content)\n",
    "\n",
    "main_folder_name = 'Images'\n",
    "if not os.path.isdir(main_folder_name):\n",
    "    os.makedirs(main_folder_name)\n",
    "\n",
    "players = [\"lionel_messi\", \"cristiano_ronaldo\", \"charles_leclerc\", \"alex_morgan\"]\n",
    "for j in range(len(players)):\n",
    "    #creating player directory\n",
    "    sub_folder_name = players[j]\n",
    "    if not os.path.isdir(os.path.join(main_folder_name, sub_folder_name)):\n",
    "        os.makedirs(os.path.join(main_folder_name, sub_folder_name))\n",
    "    player = players[j].split(\"_\")  # #creating search string\n",
    "    search_string = player[0]+\"+\"+player[1]+\"+\"+\"face\"\n",
    "\n",
    "    url = f\"https://www.google.com/search?q={search_string}&source=lnms&tbm=isch\"\n",
    "    driver.get(url)\n",
    "    time.sleep(7)\n",
    "    for i in range(1, 55):\n",
    "        img_url = \"\"\n",
    "        #At div 25 we do not have images.\n",
    "        if i % 25 != 0:\n",
    "            try:\n",
    "                image_container = driver.find_element(By.XPATH,f'//*[@id=\"islrg\"]/div[1]/div[{i}]/a[1]/div[1]/img').click()\n",
    "                time.sleep(3)\n",
    "                img_url = driver.find_element(By.XPATH,'//*[@id=\"Sva75c\"]/div[2]/div[2]/div[2]/div[2]/c-wiz/div/div/div/div[3]/div[1]/a/img[1]').get_attribute(\"src\")\n",
    "                print(img_url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print(\"Could not click\")\n",
    "            try:\n",
    "                if img_url !=\"\":\n",
    "                    download_image(img_url, os.path.join(main_folder_name, sub_folder_name), i)\n",
    "            except:\n",
    "                print(\"Could not download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6988ea-d4cc-44c8-87d5-84c7dc864c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('opencv/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('opencv/haarcascades/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afe869-b4b1-4a11-a0ae-d6f9a67ac6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapped_data = \"Images/\"\n",
    "dataset_path = \"cropped_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8287e8-f4f6-410a-9ce5-53ba94877ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = []\n",
    "names = {}\n",
    "cropped_image_dirs = []\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "     shutil.rmtree(dataset_path)         #delete current contents of the folder if folder exists\n",
    "\n",
    "for img_dir in img_dirs:\n",
    "    name = img_dir.split('/')[-1]        #get celebrity name from it directory\n",
    "    print(name)\n",
    "    names[name] = []\n",
    "    c=1\n",
    "    for entry in os.scandir(img_dir):\n",
    "        path = entry.path\n",
    "        path_str = re.sub(r'\\\\', '/', path)        #replace double backslash with frontslash to get image path\n",
    "        img = cv2.imread(path_str)\n",
    "        print(path_str)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   #convert to greyscale\n",
    "        faces_detected = face_cascade.detectMultiScale(gray, 1.3, 5)  #detect faces\n",
    "        for (x,y,w,h) in faces_detected:\n",
    "            gray_img_face = gray[y:y+h, x:x+w]                        #crop face in greyscale and colored image\n",
    "            colored_face = img[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(gray_img_face)        #detect eyes to ensure image is clear\n",
    "            if len(eyes) >= 2:                                        #only add image in dataset if full face with eyes visib\n",
    "                new_dataset = dataset_path + name\n",
    "                if not os.path.exists(new_dataset):\n",
    "                    os.makedirs(new_dataset)\n",
    "                    cropped_image_dirs.append(new_dataset)\n",
    "                cropped_face = name + str(c) + \".jpg\"\n",
    "                file_path = new_dataset + \"/\" + cropped_face\n",
    "                \n",
    "                cv2.imwrite(file_path, colored_face)\n",
    "                names[name].append(file_path)\n",
    "                c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951eda00-57bf-4541-9021-8982df61d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = \"cropped_dataset/\"\n",
    "\n",
    "\n",
    "rotation_angles = [90, -90, 180]\n",
    "\n",
    "for subfolder in os.listdir(input_dir):\n",
    "    name = subfolder.split('/')[-1]        #get celebrity name from it directory\n",
    "    print(name)\n",
    "    subfolder_path = os.path.join(input_dir, subfolder)\n",
    "    print(subfolder_path)\n",
    "  \n",
    "    for filename in os.listdir(subfolder_path):\n",
    "    \n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(subfolder_path, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "        angle = random.choice(rotation_angles)  # Applying augmentation\n",
    "        if angle == 90:\n",
    "            rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif angle == -90:\n",
    "            rotated_image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        elif angle == 180:\n",
    "             rotated_image = cv2.rotate(image, cv2.ROTATE_180)\n",
    "       \n",
    "        output_filename = f\"{os.path.splitext(filename)[0]}_aug_.jpg\"\n",
    "        output_path = os.path.join(subfolder_path, output_filename)\n",
    "        names[name].append(output_path)\n",
    "        cv2.imwrite(output_path, rotated_image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ca6554-580f-45d8-8d90-e2ec0911f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of '.ipynb_checkpoints/ML-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'ML.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    }
   ],
   "source": [
    "!git add . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b13535-7d4b-4742-8be5-fe584312e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main a08b07e] Data Augmentation added\n",
      " 2 files changed, 90 insertions(+), 68 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Data Augmentation added\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87760580-c386-48ad-8ced-33a814ffbdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/Eesa-Sarosh/Sports-Player-Classifier.git\n",
      "   fcfea03..a08b07e  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4822f4c-d134-48f6-a11d-cda9eb0d8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in names.items():\n",
    "    updated_value = [path.replace('\\\\', '/') for path in value]\n",
    "    names[key] = updated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb51c7-d78b-4f94-9559-de00acf1ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972cc69-f915-400e-9972-517b942dd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "import cv2    \n",
    "\n",
    "def w2d(img, mode='haar', level=1): #Function to extract features from an image\n",
    "\n",
    "    imArray = img\n",
    "    #Datatype conversions\n",
    "    #convert to grayscale\n",
    "    imArray = cv2.cvtColor( imArray,cv2.COLOR_RGB2GRAY )\n",
    "    #convert to float\n",
    "    imArray =  np.float32(imArray)   \n",
    "    imArray /= 255;\n",
    "    # compute coefficients \n",
    "    coeffs=pywt.wavedec2(imArray, mode, level=level)\n",
    "\n",
    "    #Process Coefficients\n",
    "    coeffs_H=list(coeffs)  \n",
    "    coeffs_H[0] *= 0;  \n",
    "\n",
    "    # reconstruction\n",
    "    imArray_H=pywt.waverec2(coeffs_H, mode);\n",
    "    imArray_H *= 255;\n",
    "    imArray_H =  np.uint8(imArray_H)\n",
    "\n",
    "    return imArray_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957a1be-e6f2-4972-8ac9-458a097fed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('cropped_dataset/cristiano_ronaldo/cristiano_ronaldo4.jpg')\n",
    "im_har = w2d(img,'db1',5)\n",
    "plt.imshow(im_har, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12327eca-80c1-434b-83ee-a03874287426",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_map = {}\n",
    "count = 0\n",
    "for celebrity_name in names.keys():\n",
    "    names_map[celebrity_name] = count\n",
    "    count = count + 1\n",
    "names_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14102ca-8724-498a-ae28-1ed2ddc66ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for celebrity_name, img_path in names.items():\n",
    "    for file in img_path:\n",
    "        img = cv2.imread(file)\n",
    "        if img is not None:\n",
    "            raw_img = cv2.resize(img, (32, 32)) #setting image size to 32x32\n",
    "            img_har = w2d(img,'db1',5) #getting wavelet transformed image\n",
    "            wave_img = cv2.resize(img_har, (32, 32))\n",
    "            combined_img = np.vstack((raw_img.reshape(32*32*3,1),wave_img.reshape(32*32,1))) #Stacking the image one to another for input\n",
    "            X.append(combined_img)\n",
    "            y.append(names_map[celebrity_name])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb5d0d-f4d3-4085-886c-5316ddf40010",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(len(X),4096).astype(float)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d897bf-cf35-438e-a21e-d61ace6b8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824fab2-cc4b-44b2-ad7b-d229dff0146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Preprocess the data scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#train the SVM model\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Assess the model performance on the test set\n",
    "accuracy = svc.score(X_test_scaled, y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a34d6c-8358-4992-b976-c7a5a51fe54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "# Preprocess the data - scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "# Instantiate logistic regression model\n",
    "logistic_regression = LogisticRegression(max_iter=1000,C=.5, solver='lbfgs')\n",
    "\n",
    "# Train the model\n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = logistic_regression.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0dc90-62b2-4f69-95a6-cf1ccb2eff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creating Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "\n",
    "# Training the Random Forest classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cc9c2-bcbc-4fa5-845c-2653181e28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# Split the data into training and testing sets\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "# Plot confusion matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8d688-e092-4778-a081-d4b7369f3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib\n",
    "import joblib \n",
    "joblib.dump(logistic_regression, 'saved_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e09d8-dcce-4f7a-8e42-63f717b181fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"class_dictionary.json\",\"w\") as f:\n",
    "    f.write(json.dumps(names_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84daebf0-2b2c-4cf2-9a6b-7d21abf66d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
